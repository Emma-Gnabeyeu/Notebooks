{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7d5a96",
   "metadata": {
    "id": "de7d5a96"
   },
   "source": [
    "#### Projects MAP 556\n",
    "### Monte Carlo methods and stochastic processes\n",
    "##### GNABEYEU MBIADA Emmanuel\n",
    "##### ALIOUA Imrane \n",
    "\n",
    "## Variance Reduction Applied to Machine Learning for Pricing Bermudan/American Options in High Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60471de",
   "metadata": {
    "id": "b60471de"
   },
   "source": [
    "> We implement here by a backward dynamic programming algorithm, an efficient method to compute the price of multi-asset American options following a Black-Scholes dynamics, based on Machine Learning, Monte Carlo simulations and variance reduction technique. \n",
    "\n",
    "> The backward dynamic programming algorithm considers a finite number of uniformly distributed exercise dates. On these dates, the option value is computed as the maximum between the exercise value and the continuation value, which is obtained by means of Gaussian process regression technique and Monte Carlo simulations. \n",
    "\n",
    "\n",
    "> We will show that such a method performs well for low dimension baskets but it is not accurate for very high dimension baskets. In order to improve the dimension range and then  overcome the\n",
    "problem of the curse of dimensionality, we employ the European option price as a control variate, which allows us to treat very large baskets and moreover to reduce the variance of price estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g84qEX6BFkCz",
   "metadata": {
    "id": "g84qEX6BFkCz"
   },
   "source": [
    "### Useful Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "238d18fe",
   "metadata": {
    "id": "238d18fe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm \n",
    "from scipy.stats import norm as scnorm\n",
    "#import plotly.graph_objects as go\n",
    "from ipywidgets import interact, widgets\n",
    "from scipy.stats import qmc\n",
    "from scipy import linalg \n",
    "import scipy.optimize as opt\n",
    "\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af8d4ed6",
   "metadata": {
    "id": "af8d4ed6"
   },
   "outputs": [],
   "source": [
    "# Supressing the warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CS1Twd9OEkIc",
   "metadata": {
    "id": "CS1Twd9OEkIc"
   },
   "source": [
    "### American options in the multi-dimensional Black-Scholes model\n",
    "An American option with maturity T is a derivative instrument whose holder can exercise the intrinsic\n",
    "optionality at any moment, from inception up to maturity. \\\\\n",
    " Let $S = (S_t)$ $t \\in [0,T]$ denote the d-dimensional underlying process. Such a stochastic process is assumed to randomly evolve according to the multidimensional Black-Scholes model: \n",
    " \n",
    "Under the risk neutral measure, such a model is given by the following equation\n",
    "$dS^i_t = (r − \\eta_i) S^i_t dt + \\sigma_i S^i_tdW^i_t$ \n",
    ", i = 1, . . . , d, \n",
    "with $S_0 = (s_{0,1}, . . . , s_{0,d})^⊤\\in \\mathbb{R}^d_+$ the spot price, r the (constant) spot interest rate, $\\eta = (\\eta_1, . . . , \\eta_d)^⊤ $the\n",
    "vector of (constant) dividend rates, $\\sigma = (\\sigma_1, . . . , \\sigma_d)^⊤$\n",
    "the vector of (constant) volatilities,\n",
    "\n",
    " W a d-dimensional\n",
    "correlated Brownian motion and $ρ_{ij}$ the instantaneous correlation coefficient between $W^i_t$\n",
    "and $W^j_t$. \n",
    "\n",
    "Moreover,\n",
    "let $\\Psi(S_T)$ denote the cash-flow associated with the option. Thus, the price at time t of an American option\n",
    "having maturity T and payoff function $\\Psi : \\mathbb{R}^d_+ \\rightarrow \\mathbb{R}$ is then\n",
    "$ v^{AM}(t, x) = sup_{τ \\in \\mathbb{T}_{t,T}} \\mathbb{E}_{t,x}[e^{−r(τ−t)}Ψ (S_τ )]$\n",
    "\n",
    "For simulation purposes, the d−dimensional Black-Scholes model can be written alternatively using the Cholesky decomposition. Specifically, for $i \\in {1, . . . , d}$ we can write\n",
    "\n",
    "$dS^i_t =S^i_t( (r − \\eta_i)  dt + \\sigma_i \\Sigma_i dB^i_t)$ \n",
    "\n",
    "where B is a d-dimensional Brownian motion and $\\Sigma_i$\n",
    "is the i-th row of the matrix $\\Sigma_i $ defined as a square root(Cholesky decomposition)\n",
    "of the correlation matrix $\\Gamma$ given by $\\Gamma_{ij}= \\rho_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4Puq_OvfF6Uu",
   "metadata": {
    "id": "4Puq_OvfF6Uu"
   },
   "source": [
    "#### CoVariance and Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14f26037",
   "metadata": {
    "id": "14f26037"
   },
   "outputs": [],
   "source": [
    "def variance(sigma_i,d):\n",
    "    # compute sigma\n",
    "    return np.array([sigma_i for _ in range(d)])\n",
    "\n",
    "def sqrtmh(A):\n",
    "    vals, vecs = linalg.eigh(A)\n",
    "    return vecs @ np.diag(np.sqrt(vals)) @ vecs.T.conjugate()\n",
    "\n",
    "def square_root_correlation_matrix(rho,d,sigma_i):\n",
    "    # compute Gamma and it square gamma\n",
    "    Gamma=rho*np.ones(d)+(1-rho)*np.eye(d)\n",
    "    CorrMatrix=(sigma_i**2)*Gamma\n",
    "    return sqrtmh(Gamma),CorrMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88abf94",
   "metadata": {
    "id": "f88abf94"
   },
   "source": [
    "#### Geometric and Arithmetic Basket Put Options and  Maximum Call Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1028feb4",
   "metadata": {
    "id": "1028feb4"
   },
   "outputs": [],
   "source": [
    "class BasketOptions:\n",
    "    def __init__(self, Spot , Strike: float ):\n",
    "        self.S = Spot\n",
    "        self.K = Strike\n",
    "        \n",
    "    def __Arithmetic_Basket_Put_Options__(self,S,K):\n",
    "        \"\"\" S: array of side (d,1)\n",
    "        \"\"\"\n",
    "        self.__init__(self, S , K )\n",
    "        return np.maximum(self.K-self.S.mean(),0)\n",
    "    \n",
    "    def Call_Maximum_d_assets_American_option(self,S,K):\n",
    "        \"\"\" S: array of side (d,1)\n",
    "        \"\"\"\n",
    "        self.__init__(self, S , K )\n",
    "        return np.maximum(self.S.max()-self.K,0)\n",
    "    \n",
    "    def Geometric_Basket_Put_Options(self,S,K):\n",
    "        \"\"\" S: array of side (d,1)\n",
    "        \"\"\"\n",
    "        self.__init__(self, S , K )\n",
    "        prod=1\n",
    "        d=self.S.shape[0]\n",
    "        for i in range(d):\n",
    "            prod*=S[i]\n",
    "            u= K-prod**(1/d)\n",
    "        return np.maximum(u,0)\n",
    "\n",
    "    \n",
    "def Geometric_Basket_Put_Options(S,K):\n",
    "    \"\"\" S: array of side (d,1)\n",
    "    \"\"\"\n",
    "    d=S.shape[0]\n",
    "    return max(K - np.prod(S)**(1 / d), 0)\n",
    "\n",
    "def Arithmetic_Basket_Put_Options(S,K):\n",
    "    \"\"\" S: array of side (d,1)\n",
    "    \"\"\"\n",
    "    return np.maximum(K-S.mean(), 0)\n",
    "\n",
    "\n",
    "def Call_Maximum_d_assets_American_option(S,K):\n",
    "    \"\"\" S: array of side (d,1)\n",
    "    \"\"\"\n",
    "    return np.maximum(np.maximum(S) - K, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aec689",
   "metadata": {
    "id": "36aec689"
   },
   "source": [
    "## Halton sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "G7y6i-MHKIqh",
   "metadata": {
    "id": "G7y6i-MHKIqh"
   },
   "outputs": [],
   "source": [
    "def next_prime():\n",
    "    def is_prime(num):\n",
    "        \"Checks if num is a prime value\"\n",
    "        for i in range(2,int(num**0.5)+1):\n",
    "            if(num % i)==0: return False\n",
    "        return True\n",
    "\n",
    "    prime = 3\n",
    "    while(1):\n",
    "        if is_prime(prime):\n",
    "            yield prime\n",
    "        prime += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "v6SiTK0xKGrR",
   "metadata": {
    "id": "v6SiTK0xKGrR"
   },
   "outputs": [],
   "source": [
    "def vdc(n, base=2):\n",
    "    vdc, denom = 0, 1\n",
    "    while n:\n",
    "        denom *= base\n",
    "        n, remainder = divmod(n, base)\n",
    "        vdc += remainder/float(denom)\n",
    "    return vdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a2f03e",
   "metadata": {
    "id": "c6a2f03e"
   },
   "outputs": [],
   "source": [
    "def halton_sequence(size, dim):\n",
    "    seq = []\n",
    "    primeGen = next_prime()\n",
    "    next(primeGen)\n",
    "    for d in range(dim):\n",
    "        base = next(primeGen)\n",
    "        seq.append([vdc(i, base) for i in range(size)])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ut-azyvZX0Vv",
   "metadata": {
    "id": "ut-azyvZX0Vv"
   },
   "source": [
    "# Geometric and Arithmetic Basket Put Options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rJ_fZQkHXxMX",
   "metadata": {
    "id": "rJ_fZQkHXxMX"
   },
   "outputs": [],
   "source": [
    "## Geometric and Arithmetic Basket Put Options.\n",
    "# Define dimension.\n",
    "d = 2 #2, 5, 10, 20, 40, 100\n",
    "# Maturity.\n",
    "T = 1\n",
    "# S\n",
    "Szero = 100\n",
    "# Strike. \n",
    "K = 100\n",
    "# Returns.\n",
    "r = 0.05\n",
    "# Dividend Rates.\n",
    "eta = 0\n",
    "# Standard Deviation.\n",
    "dev_std = 0.2\n",
    "# Correlations\n",
    "rho = 0.2\n",
    "# Number of exercise dates.\n",
    "N = 10\n",
    "# Number of points.\n",
    "P = 250 #250, 500, 1000\n",
    "# Number of Monte Carlo simulations.\n",
    "M = 10**3 #10**3, 10**4, 10**5\n",
    "# Points for the computation of the European prices with GRP-EI\n",
    "Q = 10000\n",
    "# Halton Process.\n",
    "sample = np.array(halton_sequence(Q, d))\n",
    "\n",
    "\n",
    "# Number of time steps.\n",
    "delta_t = T/N\n",
    "# Discrete exercise dates.\n",
    "t = np.array([i * delta_t for i in range(1,N+1)])\n",
    "\n",
    "G_n_p_m = np.array([[[np.random.normal(size = d) for _ in range(M)] for _ in range(P)] for _ in range(N)])\n",
    "gamma_corr = np.eye(d) * (1-rho) + np.ones((d,d)) * rho\n",
    "sigma_maj_mat = np.sqrt(gamma_corr) ## Sigma majuscule\n",
    "pi_matrix = np.ones((d,d)) * rho * dev_std "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l1ZzSINVcBES",
   "metadata": {
    "id": "l1ZzSINVcBES"
   },
   "source": [
    "\n",
    "\n",
    "# Call on the Maximum option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d_iZQNOZb736",
   "metadata": {
    "id": "d_iZQNOZb736"
   },
   "outputs": [],
   "source": [
    "## Call on the Maximum option.\n",
    "# Dimension.\n",
    "d = 2 #2, 5, 10, 20, 30, 50, 100\n",
    "# Maturity.\n",
    "T = 3\n",
    "# S\n",
    "Szero = 100\n",
    "# Strike. \n",
    "K = 100\n",
    "# Returns.\n",
    "r = 0.05\n",
    "# Dividend Rates.\n",
    "eta = 0.1\n",
    "# Standard Deviation.\n",
    "dev_std = 0.2\n",
    "# Correlations\n",
    "rho = 0\n",
    "# Number of exercise dates.\n",
    "N = 9 #9\n",
    "# Number of points.\n",
    "P = 250 #250, 500, 1000\n",
    "# Number of Monte Carlo simulations.\n",
    "M = 10 #10**3, 10**4, 10**5\n",
    "\n",
    "\n",
    "# Number of time steps.\n",
    "delta_t = T/N\n",
    "# Discrete exercise dates.\n",
    "t = np.array([i * delta_t for i in range(1,N+1)])\n",
    "\n",
    "G_n_p_m = np.array([[[np.random.normal(size = d) for _ in range(M)] for _ in range(P)] for _ in range(N)])\n",
    "gamma_corr = np.eye(d) * (1-rho) + np.ones((d,d)) * rho\n",
    "sigma_maj_mat = np.sqrt(gamma_corr) ## Sigma majuscule\n",
    "pi_matrix = np.ones((d,d)) * rho * dev_std "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeb7e6",
   "metadata": {
    "id": "8deeb7e6"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85243c9f",
   "metadata": {
    "id": "85243c9f"
   },
   "outputs": [],
   "source": [
    "## Global variables.\n",
    "x = np.array([[[Szero * np.exp((r-eta-0.5*dev_std**2)*t[n] + np.exp(np.sqrt(t[n]) * dev_std * np.dot(sigma_maj_mat, norm.ppf(sample[:,p])))) for i in range(d)] for p in range(P)] for n in range(N)])\n",
    "x_tilde = np.array([[[x[n][p] * np.exp((r-eta-0.5*dev_std**2)*delta_t + np.exp(np.sqrt(delta_t) * dev_std * np.dot(sigma_maj_mat, norm.ppf(sample[:,p])) * G_n_p_m[n][p][m])) for m in range(M)] for p in range(P)] for n in range(N) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Pk7RO_uYtBtK",
   "metadata": {
    "id": "Pk7RO_uYtBtK"
   },
   "outputs": [],
   "source": [
    "sigma_f = 1\n",
    "l = 1\n",
    "sigma_p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lOjfSViKsadc",
   "metadata": {
    "id": "lOjfSViKsadc"
   },
   "outputs": [],
   "source": [
    "def pricing_EU_option(z, z_tilde, t, t_tilde, weight, sigma_f = sigma_f, l = l):\n",
    "  v_eu = np.exp(-r*(T-t)) * (sigma_f ** 2) * l**d / np.sqrt(np.linalg.det(np.dot(T-t, pi_matrix) + l**2 * np.eye(d))) \n",
    "  return v_eu * np.sum(weight*np.exp(-0.5 * np.dot((z-z_tilde), np.dot(np.linalg.inv(np.dot((T-t_tilde), pi_matrix + l**2 * np.eye(d))),z-z_tilde))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be204eb7",
   "metadata": {
    "id": "be204eb7"
   },
   "source": [
    "## Kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd9c3f8",
   "metadata": {
    "id": "fbd9c3f8"
   },
   "outputs": [],
   "source": [
    "def kernel_function(x, y, sigma_f=1, l=1):\n",
    "    kernel = sigma_f * np.exp(- (np.linalg.norm(x - y)**2) / (2 * l**2))\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf138d",
   "metadata": {
    "id": "7bdf138d"
   },
   "source": [
    "## Covariance Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70541c5e",
   "metadata": {
    "id": "70541c5e"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def compute_cov_matrices(x, x_star, sigma_f=1, l=1):\n",
    "    \"\"\"\n",
    "    Compute components of the covariance matrix of the joint distribution.\n",
    "    \n",
    "    We follow the notation:\n",
    "    \n",
    "        - K = K(X, X) \n",
    "        - K_star = K(X_*, X)\n",
    "        - K_star2 = K(X_*, X_*)\n",
    "    \"\"\"\n",
    "    d = x.shape[0]\n",
    "    d_star = x_star.shape[0]\n",
    "\n",
    "    K = [kernel_function(i, j, sigma_f=sigma_f, l=l) for (i, j) in itertools.product(x, x)]\n",
    "\n",
    "    K = np.array(K).reshape(d, d)\n",
    "    \n",
    "    K_star2 = [kernel_function(i, j, sigma_f=sigma_f, l=l) for (i, j) in itertools.product(x_star, x_star)]\n",
    "\n",
    "    K_star2 = np.array(K_star2).reshape(d_star, d_star)\n",
    "    \n",
    "    K_star = [kernel_function(i, j, sigma_f=sigma_f, l=l) for (i, j) in itertools.product(x_star, x)]\n",
    "\n",
    "    K_star = np.array(K_star).reshape(d_star, d)\n",
    "    \n",
    "    return (K, K_star2, K_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RpdIXngEnZqm",
   "metadata": {
    "id": "RpdIXngEnZqm"
   },
   "source": [
    "## Find hyperparameters of the Kernel\n",
    "#### Optimisation of the Log-Likedlihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WmAXWjtDnZqo",
   "metadata": {
    "id": "WmAXWjtDnZqo"
   },
   "source": [
    "The parameters $\\sigma_f^2, \\sigma_l$ of the kernel function and $\\sigma_P^2$\n",
    "of the noise are called hyperparameters and need to be estimated. A\n",
    "common approach is to consider the maximum likelihood estimates which can be obtained by maximizing\n",
    "the log-likelihood function(or minimizing the negative log-likelihood function) of the training data.\n",
    "\n",
    ">The minimize() function is shorthand for scipy.optimize.minimize(). This function returns a dictionary of objects including the solution to the optimization problem and whether the problem actually solved. The minimize function has three mandatory arguments, plus a lot of options. You can experiment with the options on the minimize() documentation page. \n",
    "\n",
    ">1. The first argument of the minimize function is the criterion function (crit() in this example) from which the minimize() function will test values of the parameters in searching for the minimum value.\n",
    ">2. The second argument is an initial guess for the values of the parameters that minimize the criterion function crit().\n",
    ">3. The third argument is the tuple of all the objects needed to solve the criterion function in crit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "TaQq53ydnZqp",
   "metadata": {
    "id": "TaQq53ydnZqp"
   },
   "outputs": [],
   "source": [
    "def Compute_K(X, sigma_f, sigma_l):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes the kernel matrix\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    X: matrix, \n",
    "    sigma_f,sigma_l: Kernel parameters\n",
    "\n",
    "    RETURNS: K(X,X)\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    K = [kernel_function(X, X, sigma_f=sigma_f, l=sigma_l) for (i, j) in itertools.product(X, X)]\n",
    "    K = np.array(K).reshape(n, n)\n",
    "    \n",
    "    return K\n",
    "\n",
    "def log_lik_norm(X,y,sigma_f,sigma_l,sigma_p):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes the negative of the log likelihood function\n",
    "    given parameters and data.\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    INPUTS:\n",
    "    X: matrix, y: vector \n",
    "    sigma_f,sigma_l: Kernel parameters\n",
    "    sigma_p: noise\n",
    "    RETURNS: neg_log_lik_val\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    K= Compute_K(X,sigma_f,sigma_l) \n",
    "    p=K.shape[0]\n",
    "    A=K + (sigma_p**2)*np.identity(p)\n",
    "    # assert(A.shape==p)\n",
    "    log_lik_val =0.5*(np.log(np.linalg.det(A))+ y.T@(np.linalg.inv(A))@ y)\n",
    "    neg_log_lik_val = -log_lik_val\n",
    "    return neg_log_lik_val\n",
    "\n",
    "def crit(params,X,y): #, *args):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes the negative of the log likelihood function\n",
    "    given parameters and data. This is the minimization problem version\n",
    "    of the maximum likelihood optimization problem\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    params = (3,) vector, ([sigma_f,sigma_l,sigma_p])\n",
    "    args   = length 2 tuple, (xvals, cutoff)\n",
    "\n",
    "    RETURNS: neg_log_lik_val\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    # X,y =args\n",
    "    sigma_f,sigma_l,sigma_p = params\n",
    "    \n",
    "    # y = args\n",
    "\n",
    "    neg_log_lik_val = log_lik_norm(X,y,sigma_f,sigma_l,sigma_p)\n",
    "    \n",
    "    return neg_log_lik_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Y2i05a0Jg0T",
   "metadata": {
    "id": "8Y2i05a0Jg0T"
   },
   "source": [
    "#### Function to compute the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "A7vmlsmpnZqt",
   "metadata": {
    "id": "A7vmlsmpnZqt"
   },
   "outputs": [],
   "source": [
    "def ComputeSigma_lfp(X,y):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes by optimization the hyperparameters of the GPR model.\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    X: matrix, y: vector \n",
    "    RETURNS: sigma_f_MLE, sigma_l_MLE,sigma_p_MLE\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    # resee the initialisation: avoid local minimum\n",
    "    sigma_l_init = np.random.uniform(0, 1)  \n",
    "    sigma_f_init=  np.random.uniform(0, 1)\n",
    "    sigma_p_init = np.random.uniform(0, 1)\n",
    "    mle_args = (X,y)\n",
    "    results = opt.minimize(crit, params_init, args=mle_args)\n",
    "    sigma_f_MLE, sigma_l_MLE,sigma_p_MLE  = results.x\n",
    "    return sigma_f_MLE, sigma_l_MLE,sigma_p_MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ae3ad",
   "metadata": {
    "id": "9d3ae3ad"
   },
   "source": [
    "##  The GPR Monte Carlo Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de8c9e",
   "metadata": {
    "id": "92de8c9e"
   },
   "source": [
    ">Let us introduce the GPR Monte Carlo approach. We approximate the price of an American option with the price of a Bermudan option on the same basket. Specifically, let N be the number of time steps and $\\Delta t = \\frac{T}{N}$ the time increment. \n",
    "\n",
    ">The discrete exercise dates are $t_n = n \\Delta t $, as n = 1, . . . , N. If x represents the vector of the underlying prices at the exercise date $t_n$, then the price of the Bermudan option $v^{BM}$ is given by: $ v^{BM}(t_n,x) = max(\\Psi(x), \\mathbb{E}_{t_n,x}[e^{−r∆t}v^{BM}(t_{n+1}, S_{t_{n+1}})])$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k9KKWaIClDDb",
   "metadata": {
    "id": "k9KKWaIClDDb"
   },
   "source": [
    "By knowing the function $v^{BM}(t_{n+1}, ·)$, one can compute $v^{BM}(t_{n}, ·)$ by approximation of the\n",
    "expectation in the above expression.\n",
    "\n",
    ">In order to do that, we consider a set $X^n$ of P points whose coordinates represent\n",
    "certain possible values for the underlyings at time $t_n$:\n",
    "$:\n",
    "X^n =\\{ {x^{n,p} = (x^{n,p}_1\n",
    ", . . . , x^{n,p}_d), p = 1, . . . , P} \\}\\subset \\mathbb{R}^d$\n",
    "\n",
    ">For each $x^{n,p} \\in X^n$ $v^{BM}(t_{n}, ·) $ is compute by means\n",
    "of a one step Monte Carlo simulation, by simulation a new set\n",
    "$ \\widetilde{X}^{n,p} =\\{ {\\widetilde{x}^{n,p,m} = (\\widetilde{x}^{n,p,m}_1\n",
    ", . . . , \\widetilde{x}^{n,p,m}_d), m = 1, . . . , M} \\}\\subset \\mathbb{R}^d $\n",
    "\n",
    "Then, the option price can be approximated for each $x^{n,p} \\in X^n$ by: $ \\hat{v}^{BM}(t_n,x^{n,p}) = max(\\Psi(x^{n,p}), \\frac{e^{−r∆t}}{M} \\sum_{m=1}^M v^{BM}(t_{n+1}, \\widetilde{x}^{n,p,m}))$ \n",
    "\n",
    "Of course if the quantities $v^{BM}(t_{n+1}, \\widetilde{x}^{n,p,m})$ are known for all of these simulated points $\\widetilde{x}^{n,p,m}$ \n",
    "\n",
    "After that we proceed backward(Dynamic programming or Bellman approach), the function $v^{BM} (t, ·)$ is known for t = T since it is equal to the payoff function $\\Psi (·)$ and thanks to the above result, it is known, through an approximation for the other t.\n",
    "\n",
    "Unfortunately,$v^{BM}(t_{n+1}, \\widetilde{x}^{n,p,m})$  is not known for all $t_n$, instead we will aproximate it over the set $ \\widetilde{X}^{n,p}$ (our data) by means of GPR.\n",
    "\n",
    ">Then we finally obtain the following expression: $ \\hat{v}^{BM}(t_{n-1},x^{n-1,p}) = max(\\Psi(x^{n-1,p}), \\frac{e^{−r∆t}}{M} \\sum_{m=1}^M v^{BM,GPR}_n( \\widetilde{x}^{n-1,p,m}))$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruPrkNvP3L6p",
   "metadata": {
    "id": "ruPrkNvP3L6p"
   },
   "source": [
    "#### GPR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "DRL4PPPkE70h",
   "metadata": {
    "id": "DRL4PPPkE70h"
   },
   "outputs": [],
   "source": [
    "def compute_gpr_parameters(K, K_star2, K_star, sigma_p = sigma_p ):\n",
    "    \"\"\"Compute gaussian regression parameters.\"\"\"\n",
    "    d = K.shape[0]\n",
    "    # Weight.\n",
    "    weight = np.dot(np.linalg.inv(K + (sigma_p**2)*np.eye(P)), y)\n",
    "    # Mean.\n",
    "    f_bar_star = np.dot(K_star, weight)\n",
    "    # Covariance.\n",
    "    cov_f_star = K_star2 - np.dot(K_star, np.dot(np.linalg.inv(K + (sigma_p**2)*np.eye(d)), K_star.T))\n",
    "    \n",
    "    return (f_bar_star, cov_f_star, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bYq2-ypwttfA",
   "metadata": {
    "id": "bYq2-ypwttfA"
   },
   "outputs": [],
   "source": [
    "def GPR_MC_Method(func, S0, strike_K):\n",
    "    # gamma is the squarre root of Gamma the Correlation Matrix\n",
    "    # we can proceed by a backward computation of v_BM_tilde \n",
    "\n",
    "    v_bm_dyn = np.array([func(x[N-1][p], strike_K) for p in range(P)])\n",
    "\n",
    "    for n in range(N-2, 0, -1):\n",
    "        for p in range(P):\n",
    "            s = 0\n",
    "            sigma_f_MLE, sigma_l_MLE,sigma_p_MLE = ComputeSigma_lfp(x_tilde[n][p], v_bm_dyn[p])\n",
    "            for m in range(M):\n",
    "                K, K_star2, K_star = compute_cov_matrices(x[n][p], x_tilde[n][p][m], sigma_f = sigma_f_MLE, l = sigma_l_MLE)\n",
    "                # Compute posterior mean and covariance. \n",
    "                f_bar_star, cov_f_star, weight = compute_gpr_parameters(K, K_star2, K_star, sigma_p = sigma_p_MLE)\n",
    "                s += f_bar_star\n",
    "\n",
    "            v_bm_dyn[p] = np.maximum(func(x[n][p], K), s * np.exp(-r * delta_t)/M)\n",
    "        \n",
    "        s = 0\n",
    "          for m in range(M):\n",
    "            K, K_star2, K_star = compute_cov_matrices(S0, x_tilde[0][0][m], sigma_f=sigma_f, l=l)\n",
    "            f_bar_star, cov_f_star, weight = compute_gpr_parameters(K, K_star2, K_star, sigma_p)\n",
    "            s += f_bar_star\n",
    "\n",
    "    return np.maximum(func(S0, K), s * np.exp(-r * delta_t)/M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UaojlVjA2-oF",
   "metadata": {
    "id": "UaojlVjA2-oF"
   },
   "source": [
    "#### GPR from scikitlearn\n",
    "\n",
    "This first method compute the Gaussian Process Regression over the data set (X,y) by using the SciKitlearn Library.\n",
    "\n",
    "But, we also computed (see in the next section) the Gaussian Process Regression from scratch as explained in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "iVbi-i5m201h",
   "metadata": {
    "id": "iVbi-i5m201h"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "def GPR_approximation(X,y):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function train the GPR model over the dataset (X,y).\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    X: matrix, y: vector \n",
    "    RETURNS: GPR model\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    sigma_l, sigma_f,sigma_p=ComputeSigma_lfp(X,y)\n",
    "    kernel = (sigma_f**2)*RBF(sigma_l)\n",
    "    \n",
    "    gpr = GaussianProcessRegressor(kernel=kernel,\n",
    "         random_state=0).fit(X, y)\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4J7vPtXTYCuJ",
   "metadata": {
    "id": "4J7vPtXTYCuJ"
   },
   "source": [
    "#### GPR Monte Carlo Method Version 2\n",
    "Here we use the scikit learn library to implement the GPR before computing the price\n",
    "\n",
    "> Let us note that for most of the functions below, we use the vectorialisation trick of python to speed up our execution (The iterative version which of course are too slow while executing are commented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m8iuBUZEZRLQ",
   "metadata": {
    "id": "m8iuBUZEZRLQ"
   },
   "source": [
    "#####  Choice of the set $X^n$\n",
    ">here we use a deterministic space-filling sequence based on the Halton sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1UysHIyTZX9E",
   "metadata": {
    "id": "1UysHIyTZX9E"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "def compute_x_n(S0,r,Neta, Sigma,gamma,tn,P): # For a fixed n\n",
    "    \"\"\"\n",
    "    Compute set x_np for tn.\n",
    "    Args:\n",
    "        S0: spot price\n",
    "        Neta: array of dividend\n",
    "        r:interest rate\n",
    "        Sigma:array of volatilities of assets\n",
    "        gamma : square of the covariance matrix\n",
    "        tn: time\n",
    "        P: Cardinal of the set Xn\n",
    "    Returns: X_n of shape (P,d)\n",
    "    \"\"\"\n",
    "    d=S0.shape[0]\n",
    "    sample=Halton_sequence_Rd(d,P)\n",
    "    X_n=S0 * np.exp(r-Neta-0.5*(Sigma**2)*tn + np.sqrt(tn)*Sigma*(norm.ppf(sample, loc=10, scale=2)@gamma))\n",
    "    return X_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qBwjd_cCZg5i",
   "metadata": {
    "id": "qBwjd_cCZg5i"
   },
   "source": [
    "#####  Choice of the set $X^n_p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tEIsLMD7ZpXw",
   "metadata": {
    "id": "tEIsLMD7ZpXw"
   },
   "outputs": [],
   "source": [
    "def compute_X_tilde_np(x_np,r,Neta, Sigma,gamma,DeltaT,M): # For a fixed n and a fixed p\n",
    "    \"\"\"\n",
    "    Compute the profit and loss.\n",
    "    \n",
    "    Args:\n",
    "        x_np: shape (d,1)\n",
    "    Returns:\n",
    "       X_tilde_np: shape (M,d)\n",
    "     \n",
    "    \"\"\"\n",
    "    d=x_np.shape[0]\n",
    "    G=np.random.multivariate_normal(np.zeros(d),np.identity(d),size=M)\n",
    "    X_tilde_np = x_np * np.exp(r-Neta-0.5*(Sigma**2)*DeltaT + np.sqrt(DeltaT)*Sigma*(G@gamma))\n",
    "    return X_tilde_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0HS3IRcZzuT",
   "metadata": {
    "id": "f0HS3IRcZzuT"
   },
   "source": [
    "#####  The GPR Monte Carlo Metho version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "Yjz3uBcrYC7r",
   "metadata": {
    "id": "Yjz3uBcrYC7r"
   },
   "outputs": [],
   "source": [
    "def GPR_MC_Method_Version2(func,S0,K,r,Neta, Sigma,gamma,N,T,P,M):\n",
    "    # gamma is the squarre root of Gamma the Correlation Matrix\n",
    "    # we can proceed by a backward computation of v_BM_tilde \n",
    "   \n",
    "    times = np.linspace(0, T, N, endpoint=True)\n",
    "    dt = T/N\n",
    "    # first value n=N\n",
    "    X_N=compute_x_n(S0,r,Neta, Sigma,gamma,T,P) # shape (P,d) ?\n",
    "    v_BM=[]\n",
    "    for p in range(P):\n",
    "        v_BM.append(func(X_N[p],K)) \n",
    "    v_BM=np.array(v_BM) # shape(P,1)\n",
    "    \n",
    "    # second value n=N-1\n",
    "    X_n=compute_x_n(S0,r,Neta, Sigma,gamma,times[N-1],P) # shape (P,d) ?\n",
    "    \n",
    "    v_BM_tilde=[] # the different approximation for each p\n",
    "    for p in range(P):\n",
    "        x_np=X_n[p]\n",
    "        x_tilde_np=compute_X_tilde_np(x_np,r,Neta, Sigma,gamma,dt,M) # equivalent to S_tN (M,d)\n",
    "        # MC\n",
    "        v_BM=sum([func(x_tilde_np[m],K) for m in range(M)])/M\n",
    "        v_BM_tilde.append(np.maximum(func(x_np,K),np.exp(-1*r*dt)*v_BM))\n",
    "    v_BM_tilde=np.array(v_BM_tilde)\n",
    "    \n",
    "    # train a GPR\n",
    "    gpr=GPR_approximation(X_n,v_BM_tilde)\n",
    "        \n",
    "    #intermadiate values\n",
    "    for n in range(N-2,0,-1):\n",
    "        X_n=compute_x_n(S0,r,Neta, Sigma,gamma,times[n],P) # (P,d)\n",
    "        \n",
    "        v_BM_tilde=[] # the different approximation for each p\n",
    "        for p in range(P):\n",
    "            x_np=X_n[p]\n",
    "            x_tilde_np=compute_X_tilde_np(x_np,r,Neta, Sigma,gamma,dt,M) # equivalent to Stn+1(M,d)\n",
    "            v_n=gpr.predict(x_tilde_np)\n",
    "            v_BM_tilde.append(np.maximum(func(x_np,K),np.exp(-1*r*dt)*v_n.mean()))\n",
    "        v_BM_tilde=np.array(v_BM_tilde) #.reshape(-1)\n",
    "        # train a GPR\n",
    "        # print(n)\n",
    "        gpr=GPR_approximation(X_n,v_BM_tilde)\n",
    "\n",
    "    # and then initial value:\n",
    "    x_tilde_0=compute_X_tilde_np(S0,r,Neta, Sigma,gamma,dt,M) # shape(M,d)\n",
    "    v_=gpr.predict(x_tilde_0)  \n",
    "    v_BM_tilde_0 = np.maximum(func(S0,K),np.exp(-r*dt)*v_.mean())\n",
    "  \n",
    "    return v_BM_tilde_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S-S_iKzHYzK5",
   "metadata": {
    "id": "S-S_iKzHYzK5"
   },
   "source": [
    "##### Let run an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8e0081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Halton_sequence_Rd(d,Q):\n",
    "    \"\"\"output: h shape (d,q) \n",
    "    \"\"\"\n",
    "    sampler = qmc.Halton(d=d, scramble=False) # seed=None\n",
    "    sample = sampler.random(n=Q+1)[1:]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b428aac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99069011, 0.09626292, 0.09626292],\n",
       "       [0.09626292, 0.99069011, 0.09626292],\n",
       "       [0.09626292, 0.09626292, 0.99069011]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S0=np.array([100,100,100])\n",
    "# K=100\n",
    "d=3\n",
    "sigma_i=0.2\n",
    "rho=0.2\n",
    "Neta=np.zeros(d)\n",
    "Sigma=variance(sigma_i,d)\n",
    "\n",
    "gamma,CovarMatrix=square_root_correlation_matrix(rho,d,sigma_i)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "oBBuo7JsY3jF",
   "metadata": {
    "id": "oBBuo7JsY3jF",
    "outputId": "62ea2660-f347-4bf2-d6da-84d8bc39ac47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price of the american Arithmetic_Basket_Put_Options By GPR_MC for K=110: 10.0\n"
     ]
    }
   ],
   "source": [
    "T=2\n",
    "N=10\n",
    "P=250\n",
    "M=100\n",
    "K=110\n",
    "func=Arithmetic_Basket_Put_Options #function.__Arithmetic_Basket_Put_Options__\n",
    "val=GPR_MC_Method_Version2(Arithmetic_Basket_Put_Options,S0,K,r,Neta, Sigma,gamma,N=10,T=2,P=250,M=100)\n",
    "print('Price of the american Arithmetic_Basket_Put_Options By GPR_MC for K=110:',val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0227c3f",
   "metadata": {
    "id": "a0227c3f"
   },
   "source": [
    "## The GPR Monte Carlo Control Variate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdf97f",
   "metadata": {
    "id": "5fcdf97f"
   },
   "source": [
    ">Let us present the GPR Monte Carlo Control Variate method (GPR-MC-CV), that is the proposed algorithm of the article.\n",
    "The control variate technique is commonly used to reduce the variance of Monte Carlo estimators, but it can also give its contribution in American pricing.\n",
    "\n",
    ">Let us consider an American and an European option with the same payoff function Ψ and maturity T , and let $v^{AM}$ , $v^{EU}$ denote their prices respectively. For a fixed time t and underlying stocks x, we define the American-European price gap\n",
    "as: $v(t, x) = v^{AM}(t, x) − v^{EU}(t, x)$, then v (T, x) = 0.\n",
    "\n",
    "> It is straightforward to see that $ v(t, x) = sup_{τ \\in \\mathbb{T}_{t,T}} \\mathbb{E}_{t,x}[e^{−r(τ−t)}\\hat{Ψ}(τ, Sτ )]$ where $T_t$,T stands for the set of all stopping times taking values in $[t, T ]$ and Ψ is defined by $\\hat{Ψ} (t, x) = Ψ (x) − v^{EU}(t, x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000b124",
   "metadata": {
    "id": "5000b124"
   },
   "source": [
    "Let us consider a set $Z = {z^q, q = 1, . . . , Q}$ consisting of Q points in $R^d$ quasi-randomly distributed quasi-randomly distributed\n",
    "according to the law of the vector $ (σ_1W_1^1, . . . , σ_dW_d^d)^T$\n",
    "\n",
    "In particular, we define $z^q_i =\\sqrt{T}σ_iΣ_ih^q$\n",
    "where $Σ_i$ is i-th row of the matrix Σ and is i-th row of the matrix Σ and $h^q$\n",
    "is the q-th point of the Halton sequence in $R^d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "LqDIlI-3z45J",
   "metadata": {
    "id": "LqDIlI-3z45J"
   },
   "outputs": [],
   "source": [
    "def compute_Z(T,Sigma,gamma,Q):\n",
    "    \"\"\" Sigma: (d,1)\n",
    "         Covariance: (d,d)\n",
    "    output: Z shape (Q,d) \n",
    "    \"\"\"\n",
    "    Z=[]\n",
    "    H = sample\n",
    "    Z=np.sqrt(T)*Sigma*(norm.ppf(H, loc=10, scale=2)@gamma)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4af480d1",
   "metadata": {
    "id": "4af480d1"
   },
   "outputs": [],
   "source": [
    "# Helper function to calculate the respective covariance matrices\n",
    "def cov_matrix(x1, x2, cov_function) -> np.array:\n",
    "    return np.array([[cov_function(a, b) for a in x1] for b in x2])\n",
    "\n",
    "def compute_Z(T,Sigma,gamma,Q):\n",
    "    \"\"\" Sigma: (d,1)\n",
    "         Covariance: (d,d)\n",
    "    output: Z shape (d,q) \n",
    "    \"\"\"\n",
    "    Z=[]\n",
    "    for q in range(Q):\n",
    "        z=np.sqrt(T)*Sigma*(gamma@norm.ppf(sample[q], loc=10, scale=2))\n",
    "        Z.append(Z)\n",
    "    return np.array(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c81f2e",
   "metadata": {
    "id": "46c81f2e"
   },
   "source": [
    "In a nutshell, the main idea is to approximate the function u by training the GPR method on the set Z.\n",
    "> we compute ω using the formular: $ w = (K(X,X) + σ^2_P I_P)^{−1}y$\n",
    "By cholesky decomposition, we will have: $(K(X,X) + σ^2_P I_P)^{−1}=(LL^T)^{−1}=L^{-T}L^{-1}$ and then $ w =L^{-T}L^{-1}y$, if we set $L^{-1}y=m$,  to find w, we just need to solve the equation $L^Tw=y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ec812f9",
   "metadata": {
    "id": "1ec812f9"
   },
   "outputs": [],
   "source": [
    "def approximation_u(func,S0,K,T,r,Neta,Sigma,gamma,Q):\n",
    "    y=[]\n",
    "    Z=compute_Z(T,Sigma,gamma,Q)\n",
    "    for q in range(Q):\n",
    "        z=Z[q]\n",
    "        ST=S0*np.exp((r-Neta-0.5*(Sigma**2))*T+z)\n",
    "        y.append(func(ST,K))\n",
    "    y=np.array(y)\n",
    "    sigma_f_MLE, sigma_l_MLE,sigma_p_MLE= ComputeSigma_lfp(Z,y)\n",
    "    K= Compute_K(Z,sigma_f_MLE,sigma_l_MLE)  \n",
    "    L = np.linalg.cholesky(K + sigma_p_MLE*np.eye(Q))\n",
    "    m = np.linalg.solve(L, y)\n",
    "    w=np.linalg.solve(L.T, m)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sVwX8aDqEeYk",
   "metadata": {
    "id": "sVwX8aDqEeYk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pX62GeBXTHjf",
   "metadata": {
    "id": "pX62GeBXTHjf"
   },
   "source": [
    "####  Machine Learning Exact Integration for European options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-WnXMIIUzeHM",
   "metadata": {
    "id": "-WnXMIIUzeHM"
   },
   "source": [
    " Let $u : Z \\rightarrow R$ be the function\n",
    "defined by\n",
    "$u(z) := \\Psi((S_0 exp((r − \\eta- \\frac{1}{2} \\sigma^2)T + z))$\n",
    "\n",
    "In a nutshell, the main idea is to approximate the function u by training the GPR method on the set Z.\n",
    "In particular, we employ the Squared Exponential kernel and we then\n",
    "approximate the function u(·) by\n",
    "$u^{GPR}(z) = \\sum_{q=1}^Q k_{SE}(z_q, z) ω_q$\n",
    "where $ω_1, . . . , ω_Q$ are weights of the approximation.\n",
    "\n",
    "> we compute ω using the formular: $ w = (K(X,X) + σ^2_P I_P)^{−1}y$\n",
    "\n",
    "This can be done quickly by cholesky decomposition: we will have: $(K(X,X) + σ^2_P I_P)^{−1}=(LL^T)^{−1}=L^{-T}L^{-1}$ and then $ w =L^{-T}L^{-1}y$, if we set $L^{-1}y=m$,  to find w, we just need to solve the equation $L^Tw=y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ThV0QvqzeHN",
   "metadata": {
    "id": "2ThV0QvqzeHN"
   },
   "outputs": [],
   "source": [
    "def approximation_u(func,S0,K,T,r,Neta,Sigma,gamma,Q):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes the weights of the GPR approximation model.\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    func: the basket option function\n",
    "    Sigma: vector of varaiance\n",
    "    gamma: squared root of the correlation matrix\n",
    "    RETURNS: w\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    y = []\n",
    "    Z = compute_Z(T,Sigma,gamma,Q)\n",
    "    matrix_S = S0*np.exp((r-Neta-0.5*(Sigma**2))*T+Z)\n",
    "    y = np.array([func(matrix_S[i],K) for i in range(Q)]) # Q=matrix_S.shape[0]\n",
    "    sigma_f_MLE,sigma_l_MLE,sigma_p_MLE= ComputeSigma_lfp(Z,y)\n",
    "    K = Compute_K(Z,sigma_f_MLE,sigma_l_MLE)\n",
    "    # w = np.linalg.inv(K + sigma_p_MLE*np.eye(Q))@y  \n",
    "    L = np.linalg.cholesky(K + sigma_p_MLE*np.eye(Q))\n",
    "    m = np.linalg.solve(L, y)\n",
    "    w = np.linalg.solve(L.T, m)\n",
    "\n",
    "    return w,sigma_f_MLE,sigma_l_MLE,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db51223",
   "metadata": {
    "id": "9db51223"
   },
   "source": [
    "Below, the function pricing_EU_option is the  Machine Learning Exact Integration for European options used for controle varaite.\n",
    "The function phi_tilde is used to compute the american option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d830d4",
   "metadata": {
    "id": "c9d830d4"
   },
   "outputs": [],
   "source": [
    "def pricing_EU_option(w,S0,x,Z,Q,CovarMatrix,r,Neta,t,T,sigma_f,sigma_l):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes theprice of the european option using GPR on the set Z.\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    w: weights vector\n",
    "    RETURNS: european price\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    sum_=0\n",
    "    z_tilde=np.log(x/S0)-(r-Neta-0.5*(Sigma**2))*t\n",
    "    P = (T-t)*CovarMatrix+(sigma_l**2)*np.eye(d)\n",
    "    L = np.linalg.cholesky(P)\n",
    "\n",
    "    for q in range(Q):\n",
    "        z = Z[q]\n",
    "        m = np.linalg.solve(L, (z-z_tilde))\n",
    "        factor = np.linalg.solve(L.T, m)\n",
    "        sum_+= w[q]*np.exp(-0.5*(z-z_tilde).T @ factor)\n",
    "    \n",
    "    v = (sigma_f**2)*(sigma_l**d)*sum_ / (np.linalg.det(L))**2\n",
    "    v_EU = np.exp(-r*(T-t))*v\n",
    "    return v_EU\n",
    "\n",
    "    \n",
    "\n",
    "def phi_tilde(func,w,S0,x,K,Z,Q,CovarMatrix,r,Neta,t,T,sigma_f,sigma_l):\n",
    "    return func(x,K) - pricing_EU_option(w,S0,x,Z,Q,CovarMatrix,r,Neta,t,T,sigma_f,sigma_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "vvGeayspzeHP",
   "metadata": {
    "id": "vvGeayspzeHP"
   },
   "outputs": [],
   "source": [
    "def GPR_MC_CV_algorithm(func,S0,K,r,Neta, Sigma,gamma,CovarMatrix,N,T,P,Q,M):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function computes the price of the american/bermuda option by controle variate.\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    func: the basket option function\n",
    "    RETURNS: the price at initial time\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    # gamma is the squarre root of Gamma the Correlation Matrix\n",
    "    # we can proceed by a backward computation of v_BM_tilde \n",
    "    \n",
    "    times = np.linspace(0, T, N, endpoint=True)\n",
    "    dt = T/N\n",
    "    w,sigma_f,sigma_l,Z =approximation_u(func,S0,K,T,r,Neta,Sigma,gamma,Q)\n",
    "    \n",
    "    # first value n=N useless\n",
    "    \"\"\"\n",
    "    X_np= compute_x_n(S0,r,Neta, Sigma,gamma,T,P)\n",
    "    X_tilde_np=compute_X_tilde_np(x_np,r,Neta, Sigma,gamma,dt,M)\n",
    "    v_EU=pricing_EU_option(w,P,r,t,T,d) # for each tn ?\n",
    "    \"\"\"\n",
    "\n",
    "    ###################\n",
    "    # second value n=N-1\n",
    "    X_n=compute_x_n(S0,r,Neta, Sigma,gamma,times[N-1],P) # shape (P,d) ?\n",
    "   # the different approximation for each p \n",
    "    v_BM_tilde=np.array([phi_tilde(func,w,S0,X_n[p],K,Z,Q,CovarMatrix,r,Neta,times[N-1],T,sigma_f,sigma_l) for p in range(P)])\n",
    "\n",
    "    # train a GPR\n",
    "    gpr=GPR_approximation(X_n,v_BM_tilde)\n",
    "            \n",
    "    for n in range (N-2,0,-1):\n",
    "        #Step n:\n",
    "        X_n=compute_x_n(S0,r,Neta, Sigma,gamma,times[n],P) # (P,d)\n",
    "        v_tilde=[] \n",
    "        for p in range(P):\n",
    "            x_np=X_n[p]\n",
    "            x_tilde_np=compute_X_tilde_np(x_np,r,Neta, Sigma,gamma,dt,M) # equivalent to Stn+1(M,d)\n",
    "            v_n=gpr.predict(x_tilde_np)\n",
    "        \n",
    "            phi_=phi_tilde(func,w,S0,x_np,K,Z,Q,CovarMatrix,r,Neta,times[n],T,sigma_f,sigma_l)\n",
    "            right= np.exp(-r*dt) * v_n.mean()\n",
    "            v_tilde.append(np.maximum(phi_,right))\n",
    "        v_tilde=np.array(v_tilde) #.reshape(-1)\n",
    "        # train a GPR\n",
    "        # print(n)\n",
    "        gpr=GPR_approximation(X_n,v_tilde)\n",
    "\n",
    "        \n",
    "    #step 0\n",
    "    x_tilde_0=compute_X_tilde_np(S0,r,Neta, Sigma,gamma,dt,M) # shape(M,d)\n",
    "    v_=gpr.predict(x_tilde_0)  \n",
    "    right= np.exp(-r*dt) *v_.mean()\n",
    "    v_tilde_0 = np.maximum(func(S0,K),right)  # func or phi_tilde(func,times[0],x_np,w,K) ?\n",
    "    # t=0, spot=S0\n",
    "    v_BM=v_tilde_0+pricing_EU_option(w,S0,S0,Z,Q,CovarMatrix,r,Neta,0,T,sigma_f,sigma_l)\n",
    "    return v_BM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ls3X4B0OzeHQ",
   "metadata": {
    "id": "ls3X4B0OzeHQ"
   },
   "source": [
    "##### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4YMe_GP3kZqm",
   "metadata": {
    "id": "4YMe_GP3kZqm",
    "outputId": "4ff4b175-78d1-42df-9d4d-2b31e0478562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "Price of the american Arithmetic_Basket_Put_Options By GPR_MC for K=110: 10.0\n"
     ]
    }
   ],
   "source": [
    "T=2\n",
    "N=10\n",
    "P=250\n",
    "M=100\n",
    "K=110\n",
    "func=Arithmetic_Basket_Put_Options\n",
    "val2=GPR_MC_CV_algorithm(func,S0,K,r,Neta, Sigma,gamma,CovarMatrix,N,T,P,Q,M)\n",
    "print('Price of the american Arithmetic_Basket_Put_Options By GPR_MC for K=110:',val2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "36aec689",
    "7bdf138d",
    "RpdIXngEnZqm",
    "9d3ae3ad",
    "ruPrkNvP3L6p",
    "4J7vPtXTYCuJ",
    "a0227c3f"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
